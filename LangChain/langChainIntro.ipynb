{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import langchain"
      ],
      "metadata": {
        "id": "cj_CZWKk6gjU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "udYbVAdf3_x8"
      },
      "outputs": [],
      "source": [
        "#setup API key\n",
        "\n",
        "f = open('/content/gt.txt')\n",
        "key = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain is an open-source framework that acts like a toolkit and orchestration layer for building applications with Large Language Models.\n",
        "\n",
        "It simplifies the process of chaining together different components (like LLMs, data sources, and tools) to create complex, intelligent applications. It's designed to make prompt engineering more efficient and allow developers to adapt language models to specific business contexts.\n"
      ],
      "metadata": {
        "id": "ub32B4UvUkdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(api_key = key, model = \"gemini-2.5-flash\", temperature=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mw51_hWO6FAq",
        "outputId": "1b31baf6-294b-4bc8-ab3c-a4e756b42504"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.53.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.54.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.1.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.12.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.4.56)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.12.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"what is LangChain? Explain in 200 words.\"\n",
        "\n",
        "model_response = chat_model.invoke(PROMPT)\n",
        "model_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdg8DfqL6VCC",
        "outputId": "5ba4db46-a3ea-4eb0-cf87-e67c4524ed56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It bridges the gap between raw LLMs and complex, real-world use cases. While LLMs are powerful, they often lack memory, external data access, and the ability to interact with tools.\\n\\nLangChain provides modular components to connect LLMs with external data sources and other computational steps. Its core idea revolves around \"chains\" and \"agents.\" **Chains** allow you to sequence multiple LLM calls or other utilities, like fetching information from databases, summarizing documents, or processing text in steps. **Agents** empower LLMs to reason and decide which tools to use dynamically to achieve a specific goal, such as searching the web, performing calculations, or interacting with APIs.\\n\\nIt simplifies managing prompts, handling conversational memory, and integrating diverse data types (e.g., PDFs, websites) with vector databases.\\n\\nUltimately, LangChain enables developers to build sophisticated, context-aware, and intelligent applications like advanced chatbots, personal assistants, and data analysis tools, leveraging the full potential of LLMs beyond simple text generation.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b2615-b34f-7512-9d59-420be359b8c4-0', usage_metadata={'input_tokens': 14, 'output_tokens': 1304, 'total_tokens': 1318, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1073}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OUTPUT PARSING: LLMs primarily generate free-form text. However, in many applications, you need structured data (e.g., a list of items, a JSON object, a specific format). OutputParsers bridge this gap.\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "output_parser.invoke(model_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "VaLVlfeB7DQB",
        "outputId": "c2ece6bc-5461-4d53-8f2c-bf884a347dca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It bridges the gap between raw LLMs and complex, real-world use cases. While LLMs are powerful, they often lack memory, external data access, and the ability to interact with tools.\\n\\nLangChain provides modular components to connect LLMs with external data sources and other computational steps. Its core idea revolves around \"chains\" and \"agents.\" **Chains** allow you to sequence multiple LLM calls or other utilities, like fetching information from databases, summarizing documents, or processing text in steps. **Agents** empower LLMs to reason and decide which tools to use dynamically to achieve a specific goal, such as searching the web, performing calculations, or interacting with APIs.\\n\\nIt simplifies managing prompts, handling conversational memory, and integrating diverse data types (e.g., PDFs, websites) with vector databases.\\n\\nUltimately, LangChain enables developers to build sophisticated, context-aware, and intelligent applications like advanced chatbots, personal assistants, and data analysis tools, leveraging the full potential of LLMs beyond simple text generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Template: two types- PromptTemplate and ChatPromptTemplate\n",
        "#Think of them as blueprints for your prompt templates."
      ],
      "metadata": {
        "id": "4KPpYyxeMdYF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate(\n",
        "    messages = [\n",
        "    (\"system\", \"Your are helpful AI tutor with expertise in Data Science and Artificial Intelligence\"),\n",
        "    (\"human\", \"What is {topic}? Explain in 200 words.\"),\n",
        "    ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "z93vPROcMvT_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyGVEyzUOdIy",
        "outputId": "d97bfd89-295d-47f4-b8a1-7b5bc2265527"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['topic']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.invoke({\"topic\": \"LangChain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMQP1UdbPDww",
        "outputId": "a18420e0-9eae-4b43-c7b2-9326076ab4c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Your are helpful AI tutor with expertise in Data Science and Artificial Intelligence', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain? Explain in 200 words.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_template.invoke({\"topic\": \"LangChain\"}).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acqKl9bzPKJa",
        "outputId": "df49f46a-2de5-47b5-a7c1-1d24b426c520"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Your are helpful AI tutor with expertise in Data Science and Artificial Intelligence\n",
            "Human: What is LangChain? Explain in 200 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CHAIN (|)- A chain is simply a sequence of operations where the output of one step becomes the input of the next. This allows you to build more complex and intelligent workflows than a single LLM call could achieve.\n",
        "\n",
        "The `|` symbol chains together the different components feeds the output from one component as input into the next component.\n",
        "\"\"\"\n",
        "#1 Prompt Template\n",
        "prompt =  prompt_template.invoke({\"topic\":\"Linear Regression\"})\n",
        "print(type(prompt))\n",
        "\n",
        "#2 Chat Model\n",
        "model_response = chat_model.invoke(prompt)\n",
        "print(type(model_response))\n",
        "\n",
        "#3 output Parser\n",
        "final_output = output_parser.invoke(model_response)\n",
        "print(type(final_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p8uu95BPd-M",
        "outputId": "6b404e21-1fef-48de-dec2-1fb1424f286b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "<class 'langchain_core.messages.ai.AIMessage'>\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | chat_model | output_parser\n",
        "\n",
        "user_input = {\"topic\": \"NLP\"}\n",
        "\n",
        "chain.invoke(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "MokV7ZOKYKlB",
        "outputId": "497e3f28-fbc5-47bc-ede2-042ac4717b23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural Language Processing (NLP) is a dynamic field within Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. Its primary goal is to bridge the communication gap between humans and machines, allowing computers to process and make sense of text and spoken words.\\n\\nNLP combines computational linguistics (rule-based modeling of language) with machine learning and deep learning techniques. This allows machines to \"read\" text, \"hear\" speech, decipher its meaning, extract information, determine sentiment, and even respond in a natural, human-like manner.\\n\\nKey applications of NLP are ubiquitous, including:\\n*   **Machine Translation:** Automatically converting text from one language to another (e.g., Google Translate).\\n*   **Sentiment Analysis:** Identifying the emotional tone or opinion expressed in text (e.g., customer reviews).\\n*   **Chatbots and Virtual Assistants:** Powering conversational interfaces like Siri, Alexa, or customer service bots.\\n*   **Text Summarization:** Condensing long documents into shorter, coherent versions.\\n*   **Spam Detection:** Filtering unwanted emails.\\n\\nIn essence, NLP makes technology more intuitive and responsive to the way we naturally communicate.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building a CHAT PROMPT TEMPLATE - Create an AI Tutor App that uses Prompts and Chat internally to give Python Implementation tutorial for Data Science topics\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "#Constructing System Prompt\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
        "You are friendly AI tutor with expertise in Data Science and AI who tells step by step python implementation.\n",
        "\"\"\")\n",
        "\n",
        "#Constructing Human Prompt\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"Tell me a python implementation for {topic_name}.\")\n",
        "\n",
        "#Compiling CHAT PROMPT\n",
        "chat_prompt = ChatPromptTemplate(messages=[system_prompt, human_prompt])\n",
        "\n",
        "chat_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnrXQdoEYddn",
        "outputId": "f2054a43-ee9e-4717-bbcb-cb88a24da519"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\nYou are friendly AI tutor with expertise in Data Science and AI who tells step by step python implementation.\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, template='Tell me a python implementation for {topic_name}.'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building a CHAT MODEL\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#pass the standard parameters during initialization\n",
        "chat_model = ChatGoogleGenerativeAI(api_key = key, model=\"gemini-2.5-flash\", temperature = 1)\n",
        "#print(key)"
      ],
      "metadata": {
        "id": "cwlUrSPIbxms"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building an OUPUT PARSER\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "HHyBhYKCcMUq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHAINING all components together\n",
        "\n",
        "chain = chat_prompt | chat_model | output_parser"
      ],
      "metadata": {
        "id": "FoZdiK9kcn5W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling the CHAIN\n",
        "user_input = {\"topic_name\": \"Classifiers in Python\"}\n",
        "\n",
        "output = chain.invoke(user_input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTaVX6VIc7RS",
        "outputId": "b5cb9fcb-e691-40d1-8df2-17734833d0b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there! üëã\n",
            "\n",
            "You're looking to dive into Classifiers in Python ‚Äì that's a fantastic area of Data Science! Classifiers are machine learning models used to categorize data into one of several predefined classes. Think of it like sorting emails into \"Spam\" or \"Not Spam,\" or identifying if an image contains a \"Cat\" or \"Dog.\"\n",
            "\n",
            "The go-to library for this in Python is **scikit-learn (sklearn)**. It provides a consistent API for a wide range of classification algorithms.\n",
            "\n",
            "Let's walk through a step-by-step implementation. We'll cover:\n",
            "1.  **Generating / Loading Data**: We need data to classify!\n",
            "2.  **Splitting Data**: Training and testing sets.\n",
            "3.  **Feature Scaling (Optional but Recommended)**: Standardizing our data.\n",
            "4.  **Choosing and Training a Classifier**: Fitting the model to our training data.\n",
            "5.  **Making Predictions**: Using the trained model on unseen data.\n",
            "6.  **Evaluating the Classifier**: How well did it do?\n",
            "\n",
            "We'll demonstrate with a few common classifiers:\n",
            "*   **Logistic Regression**: A linear model for binary classification (can be extended to multi-class).\n",
            "*   **Decision Tree**: A non-linear, interpretable model that makes decisions based on feature values.\n",
            "*   **Random Forest**: An ensemble method that builds multiple decision trees and combines their predictions.\n",
            "*   **Support Vector Machine (SVM)**: A powerful model that finds the optimal hyperplane to separate classes.\n",
            "\n",
            "---\n",
            "\n",
            "### Step-by-Step Python Implementation\n",
            "\n",
            "Let's get started with the code!\n",
            "\n",
            "```python\n",
            "# --- 1. Import necessary libraries ---\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import make_classification # To generate synthetic classification data\n",
            "from sklearn.model_selection import train_test_split # To split data into training and testing sets\n",
            "from sklearn.preprocessing import StandardScaler # For feature scaling\n",
            "\n",
            "# --- Classifiers we'll use ---\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.svm import SVC # Support Vector Classifier\n",
            "\n",
            "# --- Evaluation Metrics ---\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "import seaborn as sns # For pretty confusion matrices\n",
            "\n",
            "print(\"Libraries imported successfully!\")\n",
            "\n",
            "# --- 2. Generate / Load Data ---\n",
            "# For this example, we'll generate a synthetic dataset.\n",
            "# In a real-world scenario, you'd load your data using pandas (e.g., pd.read_csv()).\n",
            "X, y = make_classification(\n",
            "    n_samples=1000,        # Number of samples (rows)\n",
            "    n_features=20,         # Total number of features (columns)\n",
            "    n_informative=10,      # Number of informative features (relevant for classification)\n",
            "    n_redundant=5,         # Number of redundant features (linear combinations of informative ones)\n",
            "    n_classes=2,           # Number of target classes (binary classification in this case)\n",
            "    random_state=42        # For reproducibility\n",
            ")\n",
            "\n",
            "print(f\"\\nData Generated:\")\n",
            "print(f\"Shape of features (X): {X.shape}\") # (1000 samples, 20 features)\n",
            "print(f\"Shape of target (y): {y.shape}\")   # (1000 samples,)\n",
            "print(f\"First 5 samples of X:\\n{X[:5, :5]}\") # Display first 5 rows, first 5 columns of features\n",
            "print(f\"First 5 samples of y: {y[:5]}\")      # Display first 5 target labels\n",
            "\n",
            "# --- 3. Split Data into Training and Testing Sets ---\n",
            "# It's crucial to train on one set of data and test on another to evaluate\n",
            "# the model's ability to generalize to unseen data.\n",
            "X_train, X_test, y_train, y_test = train_test_split(\n",
            "    X, y,\n",
            "    test_size=0.2,   # 20% of data for testing, 80% for training\n",
            "    random_state=42, # For reproducibility\n",
            "    stratify=y       # Ensures the proportion of classes is the same in train and test sets\n",
            ")\n",
            "\n",
            "print(f\"\\nData Split:\")\n",
            "print(f\"X_train shape: {X_train.shape}\")\n",
            "print(f\"X_test shape: {X_test.shape}\")\n",
            "print(f\"y_train shape: {y_train.shape}\")\n",
            "print(f\"y_test shape: {y_test.shape}\")\n",
            "\n",
            "# --- 4. Feature Scaling (Optional but Recommended for some models) ---\n",
            "# Many ML algorithms perform better when numerical input variables are scaled\n",
            "# to a standard range. StandardScaler makes the mean 0 and standard deviation 1.\n",
            "# IMPORTANT: Fit scaler ONLY on X_train, then transform both X_train and X_test.\n",
            "scaler = StandardScaler()\n",
            "X_train_scaled = scaler.fit_transform(X_train)\n",
            "X_test_scaled = scaler.transform(X_test)\n",
            "\n",
            "print(f\"\\nFeatures Scaled:\")\n",
            "print(f\"First 5 samples of scaled X_train:\\n{X_train_scaled[:5, :5]}\")\n",
            "\n",
            "# --- 5. Choose and Train a Classifier & 6. Make Predictions & 7. Evaluate ---\n",
            "\n",
            "# We'll put our classifiers in a dictionary to easily iterate and compare\n",
            "classifiers = {\n",
            "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
            "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
            "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
            "    \"Support Vector Machine (SVC)\": SVC(random_state=42)\n",
            "}\n",
            "\n",
            "results = {}\n",
            "\n",
            "print(\"\\n--- Training and Evaluating Classifiers ---\")\n",
            "\n",
            "for name, model in classifiers.items():\n",
            "    print(f\"\\nTraining {name}...\")\n",
            "\n",
            "    # Train the model (fit it to the training data)\n",
            "    # Note: SVC and Logistic Regression benefit from scaled data, Decision Tree/Random Forest usually don't need it as much,\n",
            "    # but it doesn't hurt.\n",
            "    if name in [\"Logistic Regression\", \"Support Vector Machine (SVC)\"]:\n",
            "        model.fit(X_train_scaled, y_train)\n",
            "        y_pred = model.predict(X_test_scaled)\n",
            "    else:\n",
            "        model.fit(X_train, y_train) # For Decision Tree and Random Forest, unscaled data is fine\n",
            "        y_pred = model.predict(X_test)\n",
            "\n",
            "    # Make predictions on the test set\n",
            "    \n",
            "    # Evaluate the model\n",
            "    accuracy = accuracy_score(y_test, y_pred)\n",
            "    report = classification_report(y_test, y_pred, target_names=[\"Class 0\", \"Class 1\"])\n",
            "    conf_mat = confusion_matrix(y_test, y_pred)\n",
            "\n",
            "    results[name] = {\n",
            "        \"accuracy\": accuracy,\n",
            "        \"report\": report,\n",
            "        \"confusion_matrix\": conf_mat\n",
            "    }\n",
            "\n",
            "    print(f\"--- {name} Results ---\")\n",
            "    print(f\"Accuracy: {accuracy:.4f}\")\n",
            "    print(\"\\nClassification Report:\\n\", report)\n",
            "    print(\"\\nConfusion Matrix:\\n\", conf_mat)\n",
            "\n",
            "    # Plot Confusion Matrix\n",
            "    plt.figure(figsize=(6, 4))\n",
            "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
            "                xticklabels=[\"Predicted Class 0\", \"Predicted Class 1\"],\n",
            "                yticklabels=[\"Actual Class 0\", \"Actual Class 1\"])\n",
            "    plt.title(f'Confusion Matrix for {name}')\n",
            "    plt.ylabel('Actual Label')\n",
            "    plt.xlabel('Predicted Label')\n",
            "    plt.show()\n",
            "\n",
            "print(\"\\n--- All Classifiers Evaluated! ---\")\n",
            "\n",
            "# Optional: Print a summary of accuracies\n",
            "print(\"\\n--- Accuracy Summary ---\")\n",
            "for name, res in results.items():\n",
            "    print(f\"{name}: {res['accuracy']:.4f}\")\n",
            "\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Understanding the Output\n",
            "\n",
            "Let's break down the important evaluation metrics:\n",
            "\n",
            "*   **Accuracy:** The most straightforward metric. It's the ratio of correctly predicted observations to the total observations.\n",
            "    *   `accuracy_score = (TP + TN) / (TP + TN + FP + FN)`\n",
            "*   **Classification Report:** A more detailed breakdown.\n",
            "    *   **Precision:** The ratio of correctly predicted positive observations to the total *predicted* positive observations. High precision means fewer false positives.\n",
            "        *   `Precision = TP / (TP + FP)`\n",
            "    *   **Recall (Sensitivity):** The ratio of correctly predicted positive observations to the all observations in actual class. High recall means fewer false negatives.\n",
            "        *   `Recall = TP / (TP + FN)`\n",
            "    *   **F1-Score:** The harmonic mean of Precision and Recall. It tries to balance both. A good F1-score means you have low false positives and low false negatives.\n",
            "    *   **Support:** The number of actual occurrences of the class in the specified dataset.\n",
            "*   **Confusion Matrix:** A table that summarizes the performance of a classification algorithm.\n",
            "    *   **True Positive (TP):** Actual is positive, predicted is positive. (Correctly identified)\n",
            "    *   **True Negative (TN):** Actual is negative, predicted is negative. (Correctly rejected)\n",
            "    *   **False Positive (FP):** Actual is negative, predicted is positive. (Type I error - \"False Alarm\")\n",
            "    *   **False Negative (FN):** Actual is positive, predicted is negative. (Type II error - \"Miss\")\n",
            "\n",
            "### What's Next?\n",
            "\n",
            "This is a fundamental implementation! From here, you can explore:\n",
            "\n",
            "*   **Hyperparameter Tuning:** Each classifier has parameters (e.g., `n_estimators` for Random Forest, `C` for SVM) that can be optimized using techniques like Grid Search or Random Search to find the best model configuration.\n",
            "*   **Cross-Validation:** A more robust way to evaluate models by training and testing on multiple different splits of the data.\n",
            "*   **Feature Engineering:** Creating new features from existing ones to improve model performance.\n",
            "*   **More Classifiers:** Explore others like K-Nearest Neighbors, Gradient Boosting, Naive Bayes, etc.\n",
            "*   **Real-world Datasets:** Apply these techniques to actual datasets from Kaggle, UCI Machine Learning Repository, or your own projects!\n",
            "\n",
            "I hope this detailed walkthrough helps you get a solid grasp on implementing classifiers in Python! Let me know if you have any other questions or want to dive deeper into a specific aspect. Happy classifying!\n"
          ]
        }
      ]
    }
  ]
}